{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de Abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação de Pacotes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instale antes \n",
    "```bash\n",
    "brew install postgresql\n",
    "brew install ffmpeg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 # sql databse\n",
    "import pandas as pd #dataframes\n",
    "import nltk # nlp \n",
    "from nltk.corpus import stopwords\n",
    "#from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt # visualization\n",
    "from platform import python_version\n",
    "import sys,os\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.cluster import KMeans\n",
    "#from sklearn.datasets import make_blobs\n",
    "import pyttsx3\n",
    "import openai\n",
    "import subprocess\n",
    "import re\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando a versão do Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Python Info\")\n",
    "print(\"Versão: \", python_version())\n",
    "print(\"Executável: \",sys.executable)\n",
    "print(\"Detalhes: \",sys.version_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download do Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "snowball=nltk.SnowballStemmer(\"english\")\n",
    "stopwords= set(stopwords.words('english'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lê as Chaves e Passwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(\"../passwdsql.txt\",\"r\")\n",
    "passwd=file.read()\n",
    "file.close()\n",
    "file=open(\"../openaikey.txt\",\"r\")\n",
    "key=file.read()\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up the OpenAI API client :\n",
    "openai.api_key = key \n",
    "# Set up the model (more models, visit https://beta.openai.com/playground)\n",
    "model_engine = \"gpt-3.5-turbo\"\n",
    "#? document the function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conecta_db():\n",
    "    con = psycopg2.connect(host='200.145.6.178', database='dw', user='scival', password=passwd[:])\n",
    "    return con\n",
    "\n",
    "def consultar_db(sql):\n",
    "  con = conecta_db()\n",
    "  cur = con.cursor()\n",
    "  cur.execute(sql)\n",
    "  recset = cur.fetchall()\n",
    "  registros = []\n",
    "  for rec in recset:\n",
    "    registros.append(rec)\n",
    "  con.close()\n",
    "  return registros\n",
    "\n",
    "\n",
    "def gera_nouns(raw):\n",
    "  sent=nltk.sent_tokenize(raw)[1:]\n",
    "  sent2=[nltk.word_tokenize(sentence) for sentence in sent]\n",
    "  sent3=[]\n",
    "  for list in sent2:\n",
    "      sent3=[*sent3,*list]\n",
    "  sent4=[word for word in sent3 if word.isalpha()]\n",
    "  sent5=[word for word in sent4 if word not in stopwords]\n",
    "  tagged=nltk.pos_tag(sent5)\n",
    "  nouns=[tag[0] for tag in tagged if tag[1]==\"NN\" or tag[1]==\"NNS\"]\n",
    "  return [snowball.stem(noun) for noun in nouns]\n",
    "\n",
    "def function(file):\n",
    " lines = []\n",
    " for line in f:\n",
    "  lines.append(line)\n",
    " return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def txtcomp(rawtxt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a chatbot\"},\n",
    "            {\"role\": \"user\", \"content\": rawtxt},\n",
    "        ]\n",
    "            )\n",
    "    result = ''\n",
    "    for choice in response.choices:\n",
    "        result += choice.message.content\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encontrar_ods(texto):\n",
    "    ods = [\n",
    "        \"Erradicação da pobreza\",\n",
    "        \"Fome zero e agricultura sustentável\",\n",
    "        \"Saúde e bem-estar\",\n",
    "        \"Educação de qualidade\",\n",
    "        \"Igualdade de gênero\",\n",
    "        \"Água limpa e saneamento\",\n",
    "        \"Energia limpa e acessível\",\n",
    "        \"Trabalho decente e crescimento econômico\",\n",
    "        \"Indústria, inovação e infraestrutura\",\n",
    "        \"Redução das desigualdades\",\n",
    "        \"Cidades e comunidades sustentáveis\",\n",
    "        \"Consumo e produção sustentáveis\",\n",
    "        \"Ação contra a mudança global do clima\",\n",
    "        \"Vida na água\",\n",
    "        \"Vida terrestre\",\n",
    "        \"Paz, justiça e instituições eficazes\",\n",
    "        \"Parcerias e meios de implementação\"\n",
    "    ]\n",
    "    lis_ods=[]\n",
    "    for ods_texto in ods:\n",
    "        if ods_texto.lower() in texto.lower():\n",
    "            lis_ods=lis_ods+[ods_texto]\n",
    "    return lis_ods\n",
    "\n",
    "\n",
    "def gera_frequencias(lista):\n",
    "    frequencia = {}\n",
    "    for elemento in lista:\n",
    "        if elemento in frequencia:\n",
    "            frequencia[elemento] += 1\n",
    "        else:\n",
    "            frequencia[elemento] = 1\n",
    "    return(frequencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gera_ods(start,end):\n",
    "    end=df_bd.shape[0]\n",
    "    rawtxt=\"\"\" Eu preciso classificar alguns artigos com respeito as ODS da Onu.\n",
    "    aqui estão os 17 Objetivos de Desenvolvimento Sustentável (ODS) da ONU:\n",
    "\n",
    "Erradicação da pobreza\n",
    "Fome zero e agricultura sustentável\n",
    "Saúde e bem-estar\n",
    "Educação de qualidade\n",
    "Igualdade de gênero\n",
    "Água limpa e saneamento\n",
    "Energia limpa e acessível\n",
    "Trabalho decente e crescimento econômico\n",
    "Indústria, inovação e infraestrutura\n",
    "Redução das desigualdades\n",
    "Cidades e comunidades sustentáveis\n",
    "Consumo e produção sustentáveis\n",
    "Ação contra a mudança global do clima\n",
    "Vida na água\n",
    "Vida terrestre\n",
    "Paz, justiça e instituições eficazes\n",
    "Parcerias e meios de implementação.\n",
    "Com qual desses objetivos esse abstratc esta relacionado:    \n",
    "    \"\"\" \n",
    "    l=[]\n",
    "    for i in range(start,end):\n",
    "        try:    \n",
    "            output=txtcomp(rawtxt+ df_bd.loc[i][0])\n",
    "            print(output)\n",
    "            df_bd[\"OS\"].loc[i]=output\n",
    "        except:      \n",
    "            print(\"erro\")\n",
    "            wait(300)\n",
    "            print(txtcomp(rawtxt+ df_bd[\"abstract\"].loc[i]))\n",
    "            start=df_bd[df_bd[\"OS\"]!=-1].shape[0]\n",
    "            l=df_bd[df_bd[\"OS\"]!=-1][\"OS\"].tolist()\n",
    "            with open('odslistparcial.txt', 'w',encoding=\"utf-8\") as f:\n",
    "                for item in l:\n",
    "                    print(\"%s\" % item)\n",
    "                    f.write(\"%s\" % item)\n",
    "                    f.write(\"\\n\")   \n",
    "                f.close()\t\n",
    "            gera_ods(start,end)\n",
    "    return(\"fim\")\n",
    "\n",
    "# wait for 10 seconds\n",
    "def wait(seconds):\n",
    "    print(\"wait\")\n",
    "    for i in range(seconds):\n",
    "        print(seconds-i)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gera Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = consultar_db(\"\"\"select distinct abstract from scival.abstract\"\"\")\n",
    "df_bd = pd.DataFrame(reg, columns=['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bd[\"OS\"]=-1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompoe parada  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read odslistaparcial.txt file\n",
    "f = open(\"odslistparcial.txt\", \"r\", encoding=\"utf-8\")\n",
    "l = function(f)\n",
    "f.close()\n",
    "l2 = [line for line in l if line != '\\n']\n",
    "\n",
    "for i in range(0,len(l2)):\n",
    "    df_bd[\"OS\"].loc[i] = l2[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=df_bd[df_bd[\"OS\"]!=-1].shape[0]\n",
    "end=df_bd.shape[0]\n",
    "gera_ods(start,end)\n",
    "l=df_bd[df_bd[\"OS\"]!=-1][\"OS\"].tolist()\n",
    "\n",
    "#save l to a file\n",
    "with open('odslistparcial.txt', 'w',encoding=\"utf-8\") as f:\n",
    "    for item in l:\n",
    "        print(\"%s\" % item)\n",
    "        f.write(\"%s\" % item)\n",
    "        f.write(\"\\n\")   \n",
    "    f.close()\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bd[\"OS\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise das ODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=df_bd[df_bd[\"OS\"]!=-1].shape[0]\n",
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=df_bd[df_bd[\"OS\"]!=-1][\"OS\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(l) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save l to a file\n",
    "with open('odslistparcialabs.txt', 'w',encoding=\"utf-8\") as f:\n",
    "    for item in  df_bd[\"abstracts\"]n \n",
    "        print(\"%s\" % item)\n",
    "        f.write(\"%s\" % item)\n",
    "        f.write(\"\\n\")   \n",
    "    f.close()\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? 'charmap' codec can't encode characters in position 127-128: character maps to <undefined>. How to solve this error?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_ods=[encontrar_ods(str) for str in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_ods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_ods_compressed=[]\n",
    "for line in lis_ods:\n",
    "    lis_ods_compressed=lis_ods_compressed+line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lis_ods_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies=gera_frequencias(lis_ods_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort frequencies by value\n",
    "frequencies = {k: v for k, v in sorted(frequencies.items(), key=lambda item: item[1],reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = list(range(0, 17)) \n",
    "labels = list(frequencies.keys())\n",
    "plt.bar(x, frequencies.values(), color='green', linestyle='dashed', linewidth = 3, tick_label = labels)\n",
    "#rotate x axis labels\n",
    "plt.xticks(rotation=90)\n",
    "#exprt to png file with size 20x10\n",
    "plt.savefig('ods.png'   , dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuvem de Palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lisnouns=[]\n",
    "for index in range(len(df_bd)):\n",
    "    print(index)\n",
    "    lisnouns=lisnouns+gera_nouns(df_bd.iloc[index][0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lisnouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_string=(\" \").join(lisnouns)\n",
    "wordcloud = WordCloud(width = 1000, height = 500).generate(unique_string)\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"your_file_name\"+\".png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Codigo](https://cdn-images-1.medium.com/max/1200/0*gUQl9lfEA-_uJ-bW.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=len(df_bd)\n",
    "lisabs=[]\n",
    "for i in range(0,size):\n",
    "    raw=df_bd.iloc[i][0]\n",
    "    sentences = nltk.sent_tokenize(raw)[1:]\n",
    "    sentences2=(\" \").join(sentences)\n",
    "    lisabs.append(sentences2)\n",
    "    \n",
    "#Create an object\n",
    "vectorizer = TfidfVectorizer(norm = None)\n",
    "#Generating output for TF_ IDF:\n",
    "X = vectorizer.fit_transform(lisabs).toarray () #Total words with their index in model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 180\n",
    "y_pred = KMeans(n_clusters=10, n_init=5, random_state=random_state).fit_predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, frequency = np.unique(y_pred,return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_blobs(n_samples=100, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df_bd.iloc[0:1000]\n",
    "df2[\"Cluster\"]=y_pred\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = pyttsx3.init()\n",
    "voices = engine.getProperty('voices')\n",
    "print(len(voices))\n",
    "for voz in voices:\n",
    "    if voz.languages[0][0:2]=='pt':\n",
    "        print(voz.languages,voz.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.endLoop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = pyttsx3.init()\n",
    "engine.setProperty('voice', 'com.apple.voice.compact.pt-PT.Joana')\n",
    "engine.say('Pedro viu a uva')\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import os\n",
    "tts = gTTS(text='pedro viu a uva', lang='pt')\n",
    "tts.save(\"pedro.mp3\")\n",
    "os.system('mpg123 pedro.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "# Cria um parser\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Adiciona um argumento opcional '--mensagem'\n",
    "parser.add_argument('--mensagem', help='uma mensagem opcional')\n",
    "\n",
    "# Analisa os argumentos da linha de comando\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Imprime a mensagem, se foi passada\n",
    "if args.mensagem:\n",
    "    print(args.mensagem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename=\"antimanicomial.txt\"\n",
    "with open(filename, 'r') as f: #open the file\n",
    "    contents = f.read()\n",
    "\n",
    "tts = gTTS(text=contents, lang='pt')\n",
    "tts.save(\"antimanicomial.mp3\")\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "contents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "openai.api_key = \"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\"\n",
    " # Use OpenAI's API to generate embeddings for the texts\n",
    "\n",
    "texts=df_bd['abstract'][0:10]\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n",
    " \n",
    "df_bd['ada_embedding'] = df_bd.combined.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n",
    "df_bd.to_csv('output/embedded_1k_reviews.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the KMeans algorithm to cluster the texts into 2 clusters\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "clusters = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Use the AdaEmbeddings algorithm to classify the texts\n",
    "ada = AdaBoostClassifier()\n",
    "class_labels = ada.fit_predict(clusters, embeddings)\n",
    "\n",
    "# Print the class labels for the texts\n",
    "print(class_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Use the KMeans algorithm to cluster the texts into 2 clusters\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "clusters = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Use the AdaEmbeddings algorithm to classify the texts\n",
    "ada = AdaBoostClassifier()\n",
    "class_labels = ada.fit_predict(clusters, embeddings)\n",
    "\n",
    "# Print the class labels for the texts\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//generate a matplotlib barchart plot?\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "data = {'2013': {1:25,2:81,3:15}, '2014': {1:28, 2:65, 3:75}, '2015': {1:78,2:91,3:86 }}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.plot(kind='bar')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "22b54c8f1478779af2124a3dd78c54a945e4046f2670855cfe2b3d4c06c13919"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
